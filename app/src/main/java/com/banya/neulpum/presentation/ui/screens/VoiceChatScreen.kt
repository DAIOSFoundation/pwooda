package com.banya.neulpum.presentation.ui.screens

import android.Manifest
import android.content.Context
import android.content.pm.PackageManager
import androidx.compose.foundation.background
import androidx.compose.foundation.layout.*
import androidx.compose.foundation.shape.CircleShape
import androidx.compose.foundation.shape.RoundedCornerShape
import androidx.compose.material.icons.Icons
import androidx.compose.material.icons.filled.Mic
import androidx.compose.material.icons.filled.Stop
import androidx.compose.material3.*
import androidx.compose.runtime.*
import androidx.compose.runtime.DisposableEffect
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.graphics.Brush
import androidx.compose.ui.platform.LocalContext
import androidx.compose.ui.text.font.FontWeight
import androidx.compose.ui.unit.dp
import androidx.compose.ui.unit.sp
import androidx.core.content.ContextCompat
import androidx.compose.animation.core.*
import androidx.compose.foundation.Canvas
import androidx.compose.foundation.layout.size
import androidx.compose.ui.geometry.Offset
import androidx.compose.ui.graphics.drawscope.DrawScope
import androidx.compose.ui.graphics.drawscope.Stroke
import androidx.compose.ui.graphics.drawscope.rotate
import kotlin.math.cos
import kotlin.math.sin
import com.banya.neulpum.data.remote.GoogleSpeechService
import com.banya.neulpum.presentation.ui.components.EqualizerBars
import com.banya.neulpum.presentation.ui.components.EqualizerCenterReactive
import com.banya.neulpum.data.remote.VoiceWebSocketClient
import android.media.AudioAttributes
import android.media.AudioFormat
import android.media.AudioTrack
import android.media.AudioManager
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext
import kotlinx.coroutines.delay
import kotlinx.coroutines.GlobalScope
import kotlinx.coroutines.launch
import kotlinx.coroutines.CancellationException
import kotlinx.coroutines.channels.Channel
import com.banya.neulpum.presentation.ui.components.voice.VoiceCenterOverlay
import com.banya.neulpum.presentation.ui.components.voice.VoiceMicButton
import com.banya.neulpum.presentation.ui.components.voice.CircularParticleView
import com.banya.neulpum.utils.PermissionHelper
import com.banya.neulpum.utils.rememberPermissionHelper
import com.banya.neulpum.utils.rememberPermissionLauncher
import com.banya.neulpum.presentation.ui.components.MicrophonePermissionDialog
import androidx.compose.ui.viewinterop.AndroidView
import kotlin.random.Random
import android.media.audiofx.Visualizer
import kotlin.math.sqrt
import java.nio.ByteBuffer
import java.nio.ByteOrder

// PCM16LEë¥¼ WAVë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
fun pcm16leToWav(pcmData: ByteArray, sampleRate: Int = 24000, channels: Int = 1): ByteArray {
    val dataSize = pcmData.size
    val fileSize = 36 + dataSize // WAV í—¤ë”(44 bytes) - 8 bytes + dataSize
    
    val wav = ByteArray(44 + dataSize)
    val buffer = ByteBuffer.wrap(wav).order(ByteOrder.LITTLE_ENDIAN)
    
    // RIFF í—¤ë”
    buffer.put("RIFF".toByteArray())
    buffer.putInt(fileSize)
    buffer.put("WAVE".toByteArray())
    
    // fmt ì²­í¬
    buffer.put("fmt ".toByteArray())
    buffer.putInt(16) // fmt ì²­í¬ í¬ê¸°
    buffer.putShort(1.toShort()) // ì˜¤ë””ì˜¤ í¬ë§· (1 = PCM)
    buffer.putShort(channels.toShort()) // ì±„ë„ ìˆ˜
    buffer.putInt(sampleRate) // ìƒ˜í”Œ ë ˆì´íŠ¸
    buffer.putInt(sampleRate * channels * 2) // ë°”ì´íŠ¸ ë ˆì´íŠ¸
    buffer.putShort((channels * 2).toShort()) // ë¸”ë¡ ì •ë ¬
    buffer.putShort(16.toShort()) // ë¹„íŠ¸ ê¹Šì´
    
    // data ì²­í¬
    buffer.put("data".toByteArray())
    buffer.putInt(dataSize) // ë°ì´í„° í¬ê¸°
    buffer.put(pcmData) // PCM ë°ì´í„°
    
    return wav
}

@OptIn(ExperimentalMaterial3Api::class)
@Composable
fun VoiceChatScreen(
    paddingValues: PaddingValues = PaddingValues(0.dp),
    conversationId: String? = null,
    onConversationCreated: (String) -> Unit = {}
) {
    val context = LocalContext.current
    val permissionHelper = rememberPermissionHelper()
    
    var isRecording by remember { mutableStateOf(false) }
    var recordingTime by remember { mutableStateOf(0) }
    var partialText by remember { mutableStateOf("") }
    var speechService by remember { mutableStateOf<GoogleSpeechService?>(null) }
    
        // ì›Œí¬í”Œë¡œìš° ìƒíƒœ
        var currentWorkflowStep by remember { mutableStateOf("") }
        var isProcessing by remember { mutableStateOf(false) }
    
    var hasMicrophonePermission by remember {
        mutableStateOf(
            permissionHelper.isPermissionGranted(PermissionHelper.RECORD_AUDIO_PERMISSION)
        )
    }
    
    var showPermissionDialog by remember { mutableStateOf(false) }
    var showSettingsDialog by remember { mutableStateOf(false) }
    
    // ê¶Œí•œ ìš”ì²­ ëŸ°ì²˜
    val permissionLauncher = rememberPermissionLauncher { isGranted ->
        hasMicrophonePermission = isGranted
        if (!isGranted) {
            // ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆì„ ë•Œ ì„¤ì • ë‹¤ì´ì–¼ë¡œê·¸ í‘œì‹œ
            showSettingsDialog = true
        }
    }
    
    // í™”ë©´ì´ í‘œì‹œë  ë•Œë§ˆë‹¤ ê¶Œí•œ ìƒíƒœë¥¼ ë‹¤ì‹œ í™•ì¸
    LaunchedEffect(Unit) {
        hasMicrophonePermission = permissionHelper.isPermissionGranted(PermissionHelper.RECORD_AUDIO_PERMISSION)
    }
    
    
    var wsClient by remember { mutableStateOf<VoiceWebSocketClient?>(null) }
    var isWsConnected by remember { mutableStateOf(false) }
    
    // AudioTrack ì‚¬ìš© (ìŠ¤íŠ¸ë¦¬ë° ì¬ìƒ)
    var audioTrack by remember { mutableStateOf<AudioTrack?>(null) }
    val audioChunkChannel = remember { Channel<ByteArray>(capacity = Channel.UNLIMITED) }
    
    var isPlaying by remember { mutableStateOf(false) }
    var isAwaitingResponse by remember { mutableStateOf(false) }
    var audioLevel by remember { mutableStateOf(0f) }
    var visualizer by remember { mutableStateOf<Visualizer?>(null) }
    var pendingText by remember { mutableStateOf<String?>(null) }
    var doneReceived by remember { mutableStateOf(false) }
    var fallbackScheduled by remember { mutableStateOf(false) }
    var lastAudioFormat by remember { mutableStateOf<String?>(null) }
    var isWsConnecting by remember { mutableStateOf(false) }
    val mainHandler = remember { androidx.core.os.HandlerCompat.createAsync(android.os.Looper.getMainLooper()) }
    var lastRecognizedText by remember { mutableStateOf<String?>(null) }
    var playbackSessionId by remember { mutableStateOf(0) }
    // ìŒì„± ì±„íŒ…ë„ ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼í•˜ê²Œ conversationId ì‚¬ìš©
    var currentConversationId by remember { mutableStateOf(conversationId) }
    
    // SharedPreferences ì°¸ì¡° (ì „ì—­ìœ¼ë¡œ ì‚¬ìš©)
    val voiceSettingsPrefs = remember { context.getSharedPreferences("voice_settings", Context.MODE_PRIVATE) }
    
    // ë…¹ìŒ ì‹œê°„ ì—…ë°ì´íŠ¸
    LaunchedEffect(isRecording) {
        if (isRecording) {
            while (isRecording) {
                delay(1000)
                recordingTime++
            }
        } else {
            recordingTime = 0
        }
    }
    
    // ìŒì„± ì¸ì‹ ì„œë¹„ìŠ¤ ë° WebSocket ì´ˆê¸°í™”
    LaunchedEffect(Unit) {
        speechService = GoogleSpeechService(context, "YOUR_GOOGLE_API_KEY")
        
        // ì˜¤ë””ì˜¤ ë³¼ë¥¨ í™•ì¸ ë° ì„¤ì •
        val audioManager = context.getSystemService(Context.AUDIO_SERVICE) as? AudioManager
        audioManager?.let { am ->
            val currentVolume = am.getStreamVolume(AudioManager.STREAM_MUSIC)
            val maxVolume = am.getStreamMaxVolume(AudioManager.STREAM_MUSIC)
            android.util.Log.d("VoiceChatScreen", "Current media volume: $currentVolume/$maxVolume")
            if (currentVolume == 0) {
                android.util.Log.w("VoiceChatScreen", "Media volume is 0! User needs to turn up volume.")
            }
        }
        
        // WebSocket ì—°ê²° ì„¤ì •
        val prefs = context.getSharedPreferences("auth_prefs", Context.MODE_PRIVATE)
        val selectedVoiceName = voiceSettingsPrefs.getString("selected_voice_name", "leda") ?: "leda"
        val access = prefs.getString("access_token", null)
        val orgKey = prefs.getString("organization_api_key", null)
        val base = com.banya.neulpum.di.AppConfig.BASE_HOST
        val wsScheme = if (base.startsWith("https")) "wss" else "ws"
        val wsUrl = base.replaceFirst(Regex("^https?"), wsScheme) + "/ws/voice"
        // mainHandler ë° debugToastëŠ” Composable ìŠ¤ì½”í”„ì—ì„œ rememberë¡œ ê³µìœ ë¨
        
        // AudioTrack ì´ˆê¸°í™” - ë²„í¼ë¥¼ ì¶©ë¶„íˆ í¬ê²Œ ì„¤ì •í•˜ì—¬ ëŠê¹€ ë°©ì§€
        val sampleRate = 24000
        val channelConfig = AudioFormat.CHANNEL_OUT_MONO
        val audioFormat = AudioFormat.ENCODING_PCM_16BIT
        val minBufferSize = AudioTrack.getMinBufferSize(sampleRate, channelConfig, audioFormat)
        // ìµœì  ë²„í¼: minBufferSizeì˜ 4ë°° (ì•ˆì •ì ì¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì¶©ë¶„í•œ ë²„í¼)
        val bufferSize = minBufferSize * 4
        
        audioTrack = AudioTrack.Builder()
            .setAudioAttributes(
                AudioAttributes.Builder()
                    .setUsage(AudioAttributes.USAGE_MEDIA)
                    .setContentType(AudioAttributes.CONTENT_TYPE_SPEECH)
                    .build()
            )
            .setAudioFormat(
                AudioFormat.Builder()
                    .setEncoding(audioFormat)
                    .setSampleRate(sampleRate)
                    .setChannelMask(channelConfig)
                    .build()
            )
            .setBufferSizeInBytes(bufferSize)
            .setTransferMode(AudioTrack.MODE_STREAM)
            .build()
            
        // ë³¼ë¥¨ ìµœëŒ€ë¡œ ì„¤ì •
        try {
            audioTrack?.setVolume(1.0f)
            android.util.Log.d("VoiceChatScreen", "AudioTrack initialized: sampleRate=$sampleRate, bufferSize=$bufferSize, state=${audioTrack?.state}")
        } catch (e: Exception) {
            android.util.Log.e("VoiceChatScreen", "Failed to set volume", e)
        }
            
        wsClient = VoiceWebSocketClient(wsUrl, access, orgKey, object: VoiceWebSocketClient.Listener {
            override fun onOpen() {
                isWsConnected = true
                isWsConnecting = false
                // ì—°ê²° ì§€ì—° ì‹œ ë³´ë‚¸ í…ìŠ¤íŠ¸ë¥¼ ì¦‰ì‹œ ì „ì†¡
                val p = pendingText
                if (!p.isNullOrBlank()) {
                    val voiceName = voiceSettingsPrefs.getString("selected_voice_name", "leda") ?: "leda"
                    wsClient?.sendText(p, currentConversationId, voiceName) // ê¸°ì¡´ ëŒ€í™” ì‚¬ìš© (ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼)
                    pendingText = null
                }
                
            }
            override fun onTtsChunkStream(bytes: ByteArray, format: String?, chunkIndex: Int, isFinal: Boolean) {
                if (bytes.isNotEmpty()) {
                    // ì²« ì²­í¬ë¥¼ ë°›ìœ¼ë©´ ì¬ìƒ ì„¸ì…˜ ì‹œì‘
                    if (chunkIndex == 0) {
                        mainHandler.post {
                            playbackSessionId++
                            android.util.Log.d("VoiceChatScreen", "New playback session started: #$playbackSessionId")
                        }
                    }
                    
                    val enqueueResult = audioChunkChannel.trySend(bytes.copyOf())
                    if (enqueueResult.isFailure) {
                        android.util.Log.w(
                            "VoiceChatScreen",
                            "Audio chunk enqueue failed: ${enqueueResult.exceptionOrNull()?.message}"
                        )
                    } else {
                        mainHandler.post {
                            isAwaitingResponse = false
                        }
                    }
                }
                
                if (isFinal) {
                    // ë¹ˆ ë°°ì—´ì„ ì¢…ë£Œ ì‹ í˜¸ë¡œ ì „ì†¡
                    audioChunkChannel.trySend(ByteArray(0))
                    mainHandler.post {
                        doneReceived = true
                        fallbackScheduled = false
                        isAwaitingResponse = false
                    }
                }
            }
            
            override fun onTtsChunk(bytes: ByteArray, format: String?) {
                // ë ˆê±°ì‹œ ì§€ì› (í•œ ë²ˆì— ì˜¤ëŠ” ê²½ìš°)
                onTtsChunkStream(bytes, format, 0, true)
            }
                override fun onDone() {
                    println("VoiceChatScreen: onDone called")
                    android.util.Log.d("VoiceChatScreen", "onDone called")
                    // onDoneì€ ì´ì œ ìƒíƒœ ì´ˆê¸°í™”ë§Œ ë‹´ë‹¹ (ì¬ìƒì€ onTtsChunkì—ì„œ ì²˜ë¦¬)
                    doneReceived = true
                    fallbackScheduled = false
                    isAwaitingResponse = false
                    
                    // ì›Œí¬í”Œë¡œìš° ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                    currentWorkflowStep = ""
                    isProcessing = false
                    println("VoiceChatScreen: Workflow completed - step cleared, processing=false")
                    android.util.Log.d("VoiceChatScreen", "Workflow completed - step cleared, processing=false")
                    
                    // ë³´ì´ìŠ¤ ì±„íŒ… ì™„ë£Œ ì‹œ ëŒ€í™” ìƒì„± ì½œë°± í˜¸ì¶œ (ëŒ€í™”ê°€ ìƒì„±ë˜ì—ˆë‹¤ë©´)
                    if (currentConversationId != null) {
                        onConversationCreated(currentConversationId!!)
                    }
                }
            override fun onError(error: String) {
                isWsConnected = false
                isAwaitingResponse = false
                isWsConnecting = false
                android.util.Log.d("VoiceChatScreen", "WebSocket error: $error")
            }
            override fun onLog(stage: String, message: String) {
                // ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ ì—…ë°ì´íŠ¸ - ë” ì•Œì•„ë³´ê¸° ì‰½ê²Œ
                when (stage) {
                    "plan", "planner" -> {
                        currentWorkflowStep = "ğŸ¤” ìƒê° ì¤‘..."
                        isProcessing = true
                    }
                    "tool_executor" -> {
                        currentWorkflowStep = "ğŸ”§ ì‘ì—… ì¤‘..."
                        isProcessing = true
                    }
                    "summarize" -> {
                        currentWorkflowStep = "âœï¸ ë‹µë³€ ì‘ì„± ì¤‘..."
                        isProcessing = true
                    }
                    "tts" -> {
                        currentWorkflowStep = "ğŸµ ìŒì„± ë³€í™˜ ì¤‘..."
                        isProcessing = true
                    }
                    "done" -> {
                        currentWorkflowStep = ""
                        isProcessing = false
                    }
                    else -> {
                        currentWorkflowStep = when (stage) {
                            "rag" -> "ğŸ“š ì •ë³´ ê²€ìƒ‰ ì¤‘..."
                            "mcp" -> "ğŸ”— ë„êµ¬ ì‹¤í–‰ ì¤‘..."
                            "api" -> "ğŸŒ API í˜¸ì¶œ ì¤‘..."
                            "db" -> "ğŸ’¾ ë°ì´í„° ì²˜ë¦¬ ì¤‘..."
                            else -> "âš™ï¸ ì²˜ë¦¬ ì¤‘..."
                        }
                        isProcessing = true
                    }
                }
            }
            override fun onClose(code: Int, reason: String) {
                isWsConnected = false
                isAwaitingResponse = false
                isWsConnecting = false
                
            }
                override fun onConversationCreated(conversationId: String) {
                    currentConversationId = conversationId
                    onConversationCreated(conversationId)
                }
        })
        // ì—°ê²°ì€ ì¦‰ì‹œ í•˜ì§€ ì•ŠìŒ. ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œ í•„ìš” ì‹œ ì—°ê²°.
    }

    LaunchedEffect(audioTrack, playbackSessionId) {
        val track = audioTrack ?: return@LaunchedEffect
        if (playbackSessionId == 0) return@LaunchedEffect // ì´ˆê¸° ìƒíƒœì—ì„œëŠ” ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
        
        android.util.Log.d("VoiceChatScreen", "Starting playback session #$playbackSessionId")
        withContext(Dispatchers.IO) {
            try {
                var isFirstChunk = true
                val preBufferList = mutableListOf<ByteArray>()
                var totalBufferedBytes = 0
                val preBufferThreshold = 30720 // 640ms = 80ms Ã— 8ê°œ (24000Hz * 2bytes * 0.64s)
                
                // ì²­í¬ ì•ˆì •í™”: 80ms ë‹¨ìœ„ë¡œ ëª¨ì•„ì„œ write (ì„œë²„ì—ì„œ 80msë¡œ ì „ì†¡)
                val chunkAccumulator = mutableListOf<ByteArray>()
                var accumulatedBytes = 0
                val chunkWriteThreshold = 3840 // 80ms (24000Hz * 2bytes * 0.08s)
                
                for (chunk in audioChunkChannel) {
                    // ë¹ˆ ë°°ì—´ì´ë©´ ì¢…ë£Œ ì‹ í˜¸
                    if (chunk.isEmpty()) {
                        android.util.Log.d("VoiceChatScreen", "Received end-of-stream signal")
                        
                        // Pre-bufferì— ë‚¨ì€ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¬ìƒ
                        if (preBufferList.isNotEmpty()) {
                            if (track.playState != AudioTrack.PLAYSTATE_PLAYING) {
                                track.play()
                                mainHandler.post {
                                    isPlaying = true
                                    isAwaitingResponse = false
                                }
                            }
                            for (buffered in preBufferList) {
                                track.write(buffered, 0, buffered.size, AudioTrack.WRITE_BLOCKING)
                            }
                        }
                        
                        // Accumulatorì— ë‚¨ì€ ì²­í¬ë„ write
                        if (chunkAccumulator.isNotEmpty()) {
                            val mergedBuffer = ByteArray(accumulatedBytes)
                            var mergedOffset = 0
                            for (accChunk in chunkAccumulator) {
                                System.arraycopy(accChunk, 0, mergedBuffer, mergedOffset, accChunk.size)
                                mergedOffset += accChunk.size
                            }
                            track.write(mergedBuffer, 0, mergedBuffer.size, AudioTrack.WRITE_BLOCKING)
                        }
                        
                        break
                    }
                    
                    // AudioTrack ìƒíƒœ í™•ì¸
                    val state = try { track.state } catch (_: Exception) { AudioTrack.STATE_UNINITIALIZED }
                    if (state != AudioTrack.STATE_INITIALIZED) {
                        android.util.Log.w("VoiceChatScreen", "AudioTrack not initialized, stopping playback")
                        break
                    }
                    
                    // ì´ˆê¸° ë²„í¼ë§: ì²« ì²­í¬ë“¤ì„ ëª¨ì•„ì„œ í•œë²ˆì— ì¬ìƒ ì‹œì‘
                    if (isFirstChunk) {
                        preBufferList.add(chunk)
                        totalBufferedBytes += chunk.size
                        
                        if (totalBufferedBytes >= preBufferThreshold) {
                            android.util.Log.d("VoiceChatScreen", "Pre-buffering complete ($totalBufferedBytes bytes)")
                            isFirstChunk = false
                            
                            // ë¨¼ì € ë²„í¼ë§ëœ ë°ì´í„°ë¥¼ ëª¨ë‘ AudioTrackì— ì“°ê¸°
                            var totalWritten = 0
                            for (buffered in preBufferList) {
                                var offset = 0
                                while (offset < buffered.size) {
                                    val written = try {
                                        track.write(buffered, offset, buffered.size - offset, AudioTrack.WRITE_BLOCKING)
                                    } catch (e: Exception) {
                                        android.util.Log.e("VoiceChatScreen", "Write failed", e)
                                        -1
                                    }
                                    if (written <= 0) {
                                        android.util.Log.e("VoiceChatScreen", "Write returned $written, stopping")
                                        break
                                    }
                                    offset += written
                                    totalWritten += written
                                }
                            }
                            android.util.Log.d("VoiceChatScreen", "Pre-buffer written: $totalWritten bytes from ${preBufferList.size} chunks")
                            preBufferList.clear()
                            
                            // ë°ì´í„°ê°€ ì¶©ë¶„íˆ ì“°ì—¬ì§„ í›„ì— ì¬ìƒ ì‹œì‘
                            try {
                                track.play()
                                android.util.Log.d("VoiceChatScreen", "Playback started after pre-buffering")
                                mainHandler.post {
                                    isPlaying = true
                                    isAwaitingResponse = false
                                }
                            } catch (e: Exception) {
                                android.util.Log.e("VoiceChatScreen", "Failed to start playback", e)
                                break
                            }
                        }
                        continue
                    }
                    
                    // ì •ìƒ ì¬ìƒ ì¤‘: ì²­í¬ë¥¼ ëª¨ì•„ì„œ write (20~40ms ë¶„ëŸ‰)
                    if (track.playState != AudioTrack.PLAYSTATE_PLAYING) {
                        try {
                            track.play()
                            android.util.Log.d("VoiceChatScreen", "Resuming playback")
                            mainHandler.post {
                                isPlaying = true
                                isAwaitingResponse = false
                            }
                        } catch (e: Exception) {
                            android.util.Log.e("VoiceChatScreen", "Failed to start playback", e)
                            break
                        }
                    }
                    
                    // ì²­í¬ë¥¼ accumulatorì— ì¶”ê°€
                    chunkAccumulator.add(chunk)
                    accumulatedBytes += chunk.size
                    
                    // 40ms ë¶„ëŸ‰ ì´ìƒ ëª¨ì˜€ìœ¼ë©´ write
                    if (accumulatedBytes >= chunkWriteThreshold) {
                        // ëª¨ë“  ì²­í¬ë¥¼ í•©ì³ì„œ í•˜ë‚˜ì˜ ë²„í¼ë¡œ ë§Œë“¤ê¸°
                        val mergedBuffer = ByteArray(accumulatedBytes)
                        var mergedOffset = 0
                        for (accChunk in chunkAccumulator) {
                            System.arraycopy(accChunk, 0, mergedBuffer, mergedOffset, accChunk.size)
                            mergedOffset += accChunk.size
                        }
                        
                        // write
                        var offset = 0
                        while (offset < mergedBuffer.size) {
                            val written = try {
                                track.write(mergedBuffer, offset, mergedBuffer.size - offset, AudioTrack.WRITE_BLOCKING)
                            } catch (e: Exception) {
                                android.util.Log.e("VoiceChatScreen", "Write failed", e)
                                -1
                            }
                            if (written <= 0) {
                                android.util.Log.e("VoiceChatScreen", "Write returned $written at offset $offset/${mergedBuffer.size}")
                                break
                            }
                            offset += written
                        }
                        
                        // ì´ˆê¸°í™”
                        chunkAccumulator.clear()
                        accumulatedBytes = 0
                    }
                }
                
                // ì¬ìƒ ì™„ë£Œ í›„ AudioTrack ë²„í¼ê°€ ë¹„ì›Œì§ˆ ë•Œê¹Œì§€ ëŒ€ê¸°
                android.util.Log.d("VoiceChatScreen", "Waiting for playback to finish...")
                try {
                    // ë²„í¼ì— ë‚¨ì€ ë°ì´í„°ê°€ ì¬ìƒë  ì‹œê°„ í™•ë³´ (ìµœëŒ€ 1ì´ˆ)
                    var waitCount = 0
                    while (track.playState == AudioTrack.PLAYSTATE_PLAYING && waitCount < 10) {
                        delay(100)
                        waitCount++
                    }
                } catch (_: Exception) {}
                
            } catch (e: CancellationException) {
                throw e
            } catch (e: Exception) {
                android.util.Log.e("VoiceChatScreen", "Audio playback loop failed", e)
            } finally {
                android.util.Log.d("VoiceChatScreen", "Playback finished, stopping AudioTrack")
                try {
                    track.pause()
                    track.flush()
                } catch (_: Exception) {}
                mainHandler.post {
                    isPlaying = false
                    audioLevel = 0f
                }
            }
        }
    }

    DisposableEffect(Unit) {
        onDispose {
            audioChunkChannel.close()
            try { wsClient?.close() } catch (_: Exception) {}
            try { 
                audioTrack?.stop()
                audioTrack?.release() 
            } catch (_: Exception) {}
        }
    }
    
    Box(
        modifier = Modifier
            .fillMaxSize()
            .background(Color.White)
    ) {
        // ì˜¤ë””ì˜¤ ë¹„ì£¼ì–¼ë¼ì´ì € - ê³ ì • ìœ„ì¹˜ (í™”ë©´ ì¤‘ì•™ ìœ„ìª½)
        Box(
            modifier = Modifier
                .fillMaxSize()
                .padding(paddingValues),
            contentAlignment = Alignment.Center
        ) {
            // ê³ ì •ëœ ì¤‘ì•™ ìœ„ìª½ ìœ„ì¹˜
            Box(
                modifier = Modifier
                    .fillMaxWidth()
                    .offset(y = (-40).dp), // 40dp ìœ„ë¡œ ì´ë™
                contentAlignment = Alignment.Center
            ) {
                // ì˜ˆìœ ì›í˜• ì˜¤ë””ì˜¤ ë¹„ì£¼ì–¼ë¼ì´ì €
                BeautifulCircularVisualizer(
                    isActive = isRecording || isPlaying,
                    audioLevel = audioLevel,
                    modifier = Modifier.size(300.dp)
                )
            }
        }
        
        // ì›Œí¬í”Œë¡œìš° í‘œì‹œê¸° - í–„ë²„ê±°ë°”+ì œëª©ì—ì„œ 15dp ì•„ë˜ (ì˜ˆì˜ê²Œ ê¾¸ë¯¸ê¸°)
        if (currentWorkflowStep.isNotEmpty()) {
            Box(
                modifier = Modifier
                    .fillMaxSize()
                    .padding(paddingValues),
                contentAlignment = Alignment.TopCenter
            ) {
                Row(
                    modifier = Modifier
                        .fillMaxWidth()
                        .padding(horizontal = 16.dp, vertical = 8.dp)
                        .offset(y = 15.dp), // í–„ë²„ê±°ë°”+ì œëª©ì—ì„œ 15dp ì•„ë˜
                    verticalAlignment = Alignment.CenterVertically,
                    horizontalArrangement = Arrangement.Center
                ) {
                    // ê·¸ë¼ë°ì´ì…˜ ì›í˜• ë¡œë”©
                    Box(
                        modifier = Modifier
                            .size(20.dp)
                            .background(
                                brush = Brush.radialGradient(
                                    colors = listOf(
                                        Color(0xFF10A37F).copy(alpha = 0.8f),
                                        Color(0xFF10A37F).copy(alpha = 0.3f)
                                    )
                                ),
                                shape = CircleShape
                            ),
                        contentAlignment = Alignment.Center
                    ) {
                        CircularProgressIndicator(
                            modifier = Modifier.size(16.dp),
                            color = Color.White,
                            strokeWidth = 2.dp
                        )
                    }
                    Spacer(modifier = Modifier.width(12.dp))
                    Text(
                        text = currentWorkflowStep,
                        color = Color(0xFF10A37F),
                        fontSize = 15.sp,
                        fontWeight = FontWeight.SemiBold,
                        letterSpacing = 0.5.sp
                    )
                }
            }
        }
        // í•˜ë‹¨ ì˜ì—­(íŒ¨ë”© í¬í•¨) UI - ë°°ê²½ ì œê±°í•˜ì—¬ ì¤‘ì•™ ì˜¤ë²„ë ˆì´ê°€ ë³´ì´ë„ë¡
        Box(
            modifier = Modifier
                .fillMaxSize()
                .padding(paddingValues)
        ) {
            // ì „ì†¡(ë…¹ìŒ) ì¤‘ì¼ ë•Œ ë²„íŠ¼ ìœ„ ì‘ì€ ì´í€„ë¼ì´ì €
            if (isRecording) {
                Column(
                    modifier = Modifier
                        .fillMaxWidth()
                        .align(Alignment.BottomCenter)
                        .padding(bottom = 120.dp),
                    horizontalAlignment = Alignment.CenterHorizontally
                ) {
                    
                    EqualizerBars(active = true, barColor = Color(0xFF10A37F))
                    Spacer(modifier = Modifier.height(8.dp))
                }
            }

            // ìŒì„± ë…¹ìŒ FAB (í•˜ë‹¨ ì¤‘ì•™)
            VoiceMicButton(
                isRecording = isRecording,
                paddingValues = paddingValues,
                onToggle = {
                if (isRecording) {
                    isRecording = false
                    speechService?.cleanup()
                } else {
                    if (hasMicrophonePermission) {
                        // ì†Œì¼“ì´ ë¯¸ì—°ê²° ìƒíƒœì´ë©´ ìš°ì„  ì—°ê²° ì‹œë„
                        if (!isWsConnected && !isWsConnecting) {
                            isWsConnecting = true
                            android.util.Log.d("VoiceChatScreen", "WebSocket not connected, attempting to reconnect...")
                            try { 
                                // ê¸°ì¡´ ì›¹ì†Œì¼“ì´ ìˆë‹¤ë©´ ì •ë¦¬
                                wsClient?.close()
                                wsClient?.connect() 
                            } catch (e: Exception) { 
                                isWsConnecting = false
                                isWsConnected = false
                                android.util.Log.e("VoiceChatScreen", "WebSocket connection failed: ${e.message}")
                            }
                        } else if (isWsConnected) {
                            android.util.Log.d("VoiceChatScreen", "WebSocket already connected")
                        }
                        // ìƒˆ ë…¹ìŒ ì‹œì‘ ì‹œ ê¸°ì¡´ ìµœì¢… í…ìŠ¤íŠ¸ëŠ” ìˆ¨ê¹€
                        lastRecognizedText = null
                        isRecording = true
                        
                        // ë³´ì´ìŠ¤ ì±„íŒ… ì‹œì‘ ì‹œì—ëŠ” ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŒ
                        // ì„œë²„ì—ì„œ conversation_created ì´ë²¤íŠ¸ë¥¼ ë°›ì„ ë•Œë§Œ onConversationCreated í˜¸ì¶œ
                        partialText = ""
                        // ë³´ì´ìŠ¤ ìš”ì•½ í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì œê±°
                        speechService?.startSpeechRecognition(
                            onPartialResult = { text ->
                                partialText = text
                            },
                            onFinalResult = { text ->
                                isRecording = false
                                partialText = ""
                                lastRecognizedText = text
                                if (text.isNotEmpty()) {
                                    // ìƒˆ ìš”ì²­ ì‹œì‘ ì‹œ ì´ì „ ì¬ìƒ ìƒíƒœ ì™„ì „ ì´ˆê¸°í™”
                                    try { 
                                        audioTrack?.pause()
                                        audioTrack?.flush()
                                    } catch (_: Exception) {}
                                    while (audioChunkChannel.tryReceive().isSuccess) { /* drop pending audio */ }
                                    isPlaying = false
                                    audioLevel = 0f
                                    try { visualizer?.release() } catch (_: Exception) {}
                                    visualizer = null
                                    
                                    // ìƒíƒœ ë³€ìˆ˜ë“¤ ì™„ì „ ì´ˆê¸°í™”
                                    doneReceived = false
                                    fallbackScheduled = false
                                    isAwaitingResponse = true
                                    if (isWsConnected) {
                                        val voiceName = voiceSettingsPrefs.getString("selected_voice_name", "leda") ?: "leda"
                                        wsClient?.sendText(text, currentConversationId, voiceName) // ê¸°ì¡´ ëŒ€í™” ì‚¬ìš© (ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼)
                                    } else {
                                        pendingText = text
                                    }
                                }
                            },
                            onError = { _ ->
                                isRecording = false
                                partialText = ""
                                isAwaitingResponse = false
                            }
                        )
                    } else {
                        // ë§ˆì´í¬ ê¶Œí•œì´ ì—†ëŠ” ê²½ìš° ì‹¤ì œ ê¶Œí•œ ìš”ì²­
                        permissionLauncher.launch(PermissionHelper.RECORD_AUDIO_PERMISSION)
                    }
                }
                },
                modifier = Modifier
                    .align(Alignment.BottomCenter)
            )

            // ë§ˆì§€ë§‰ ìŒì„± ì¸ì‹ ê²°ê³¼ í…ìŠ¤íŠ¸ (í•˜ë‹¨ ê³ ì • í‘œì‹œ)
            if (!lastRecognizedText.isNullOrBlank()) {
                Row(
                    modifier = Modifier
                        .align(Alignment.BottomCenter)
                        .padding(bottom = 120.dp)
                ) {
                    Surface(
                        shape = RoundedCornerShape(16.dp),
                        color = Color(0xFFF7F7F8)
                    ) {
                        Text(
                            text = lastRecognizedText ?: "",
                            color = Color.Black,
                            fontSize = 16.sp,
                            modifier = Modifier.padding(horizontal = 16.dp, vertical = 10.dp)
                        )
                    }
                }
            }
        }
    }

    // ì˜¤ë””ì˜¤ ë ˆë²¨ ì‹œê°í™”: ìµœì í™”ëœ Visualizer ì‚¬ìš© (AudioTrack)
    LaunchedEffect(isPlaying, audioTrack) {
        if (isPlaying && audioTrack != null) {
            // AudioTrack ì„¸ì…˜ ID í™•ì¸
            val sessionId = try {
                audioTrack!!.audioSessionId
            } catch (e: Exception) {
                0
            }
            
            if (sessionId != 0) {
                try {
                    visualizer = Visualizer(sessionId).apply {
                        captureSize = Visualizer.getCaptureSizeRange()[0] // ìµœì†Œ í¬ê¸° ì‚¬ìš©
                        setDataCaptureListener(object : Visualizer.OnDataCaptureListener {
                            override fun onWaveFormDataCapture(v: Visualizer?, bytes: ByteArray?, samplingRate: Int) {
                                if (bytes == null) return
                                // ìµœì í™”ëœ RMS ê³„ì‚°
                                var sum = 0f
                                val len = bytes.size
                                for (i in 0 until len step 4) { // 4ë°° ìƒ˜í”Œë§ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
                                    val b = bytes[i]
                                    val fb = (b.toInt() and 0xFF) - 128
                                    sum += (fb * fb)
                                }
                                val rms = sqrt(sum / (len / 4)) / 128f
                                audioLevel = rms.coerceIn(0f, 1f)
                            }
                            override fun onFftDataCapture(v: Visualizer?, bytes: ByteArray?, samplingRate: Int) { }
                        }, Visualizer.getMaxCaptureRate() / 4, true, false) // ìº¡ì²˜ ë ˆì´íŠ¸ ê°ì†Œ
                        enabled = true
                    }
                } catch (_: Exception) {
                    // í´ë°± ì• ë‹ˆë©”ì´ì…˜ ê°„ì†Œí™”
                    while (isPlaying) {
                        audioLevel = 0.4f + Random.nextFloat() * 0.4f
                        delay(100) // ì—…ë°ì´íŠ¸ ê°„ê²© ì¦ê°€
                    }
                    audioLevel = 0f
                }
            } else {
                // í´ë°± ì• ë‹ˆë©”ì´ì…˜ ê°„ì†Œí™”
                while (isPlaying) {
                    audioLevel = 0.4f + Random.nextFloat() * 0.4f
                    delay(100) // ì—…ë°ì´íŠ¸ ê°„ê²© ì¦ê°€
                }
                audioLevel = 0f
            }
        } else {
            audioLevel = 0f
        }
    }

    DisposableEffect(Unit) {
        onDispose {
            try { visualizer?.release() } catch (_: Exception) {}
            visualizer = null
        }
    }
    
    // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ë‹¤ì´ì–¼ë¡œê·¸ (ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆì„ ë•Œ ì„¤ì •ìœ¼ë¡œ ì•ˆë‚´)
    if (showSettingsDialog) {
        MicrophonePermissionDialog(
            onConfirm = {
                showSettingsDialog = false
                permissionHelper.openAppSettings()
            },
            onDismiss = {
                showSettingsDialog = false
            }
        )
    }
    
}

@Composable
fun BeautifulCircularVisualizer(
    isActive: Boolean,
    audioLevel: Float,
    modifier: Modifier = Modifier
) {
    val infiniteTransition = rememberInfiniteTransition(label = "beautiful_visualizer")
    
    // íšŒì „ ì• ë‹ˆë©”ì´ì…˜
    val rotation by infiniteTransition.animateFloat(
        initialValue = 0f,
        targetValue = 360f,
        animationSpec = infiniteRepeatable(
            animation = tween(12000, easing = LinearEasing),
            repeatMode = RepeatMode.Restart
        ),
        label = "rotation"
    )
    
    // í„ìŠ¤ ì• ë‹ˆë©”ì´ì…˜
    // val pulse by infiniteTransition.animateFloat(
    //     initialValue = 0.9f,
    //     targetValue = 1.1f,
    //     animationSpec = infiniteRepeatable(
    //         animation = tween(3000, easing = EaseInOut),
    //         repeatMode = RepeatMode.Reverse
    //     ),
    //     label = "pulse"
    // )
    
    // ìƒ‰ìƒ ê·¸ë¼ë°ì´ì…˜ ì• ë‹ˆë©”ì´ì…˜
    val colorShift by infiniteTransition.animateFloat(
        initialValue = 0f,
        targetValue = 1f,
        animationSpec = infiniteRepeatable(
            animation = tween(4000, easing = EaseInOut),
            repeatMode = RepeatMode.Reverse
        ),
        label = "color_shift"
    )
    
    Canvas(modifier = modifier) {
        val center = Offset(size.width / 2, size.height / 2)
        val radius = minOf(size.width, size.height) / 2 * 0.85f
        
        // ê·¸ë¼ë°ì´ì…˜ ë°°ê²½ ì›
        val gradientColors = listOf(
            Color(0xFF10A37F).copy(alpha = 0.1f),
            Color(0xFF00D4AA).copy(alpha = 0.05f),
            Color(0xFF10A37F).copy(alpha = 0.1f)
        )
        
        // drawCircle(
        //     brush = Brush.radialGradient(
        //         colors = gradientColors,
        //         center = center,
        //         radius = radius * pulse
        //     ),
        //     radius = radius * pulse,
        //     center = center
        // )
        
        if (isActive) {
            // í™œì„± ìƒíƒœ - ë” ì˜ˆìœ ë°”ë“¤
            val barCount = 48
            val barWidth = 3.dp.toPx()
            
            rotate(rotation, center) {
                for (i in 0 until barCount) {
                    val angle = (i * 360f / barCount) * (Math.PI / 180f).toFloat()
                    val baseHeight = if (audioLevel > 0) {
                        (audioLevel * 80f + sin(i * 0.4f) * 30f).coerceIn(15f, 100f)
                    } else {
                        sin(i * 0.3f) * 15f + 25f
                    }
                    
                    val barHeight = baseHeight
                    val x = center.x + cos(angle) * (radius - barHeight / 2)
                    val y = center.y + sin(angle) * (radius - barHeight / 2)
                    
                    // ê·¸ë¼ë°ì´ì…˜ ìƒ‰ìƒ
                    val hue = (colorShift + i * 0.1f) % 1f
                    val barColor = when {
                        barHeight > 80f -> Color.hsl(hue * 360f, 0.7f, 0.5f)
                        barHeight > 60f -> Color.hsl(hue * 360f, 0.6f, 0.6f)
                        barHeight > 40f -> Color.hsl(hue * 360f, 0.5f, 0.7f)
                        else -> Color.hsl(hue * 360f, 0.4f, 0.8f)
                    }
                    
                    // ë‘¥ê·¼ ëª¨ì„œë¦¬ ë°”
                    drawRoundRect(
                        color = barColor,
                        topLeft = Offset(x - barWidth / 2, y - barHeight / 2),
                        size = androidx.compose.ui.geometry.Size(barWidth, barHeight),
                        cornerRadius = androidx.compose.ui.geometry.CornerRadius(barWidth / 2)
                    )
                }
            }
            
        
        } else {
            // ëŒ€ê¸° ìƒíƒœ - ë” ì˜ˆìœ ì• ë‹ˆë©”ì´ì…˜
            val dotCount = 16
            for (i in 0 until dotCount) {
                val angle = (i * 360f / dotCount) * (Math.PI / 180f).toFloat()
                val dotRadius = 6.dp.toPx()
                val dotDistance = radius * 0.75f
                
                val x = center.x + cos(angle) * dotDistance
                val y = center.y + sin(angle) * dotDistance
                
                val alpha = (sin(i * 0.3f + rotation * 0.02f) + 1f) / 2f * 0.9f + 0.1f
                val scale = (sin(i * 0.2f + rotation * 0.01f) + 1f) / 2f * 0.5f + 0.5f
                
                drawCircle(
                    color = Color(0xFF10A37F).copy(alpha = alpha),
                    radius = dotRadius * scale,
                    center = Offset(x, y)
                )
            }
            
        }
    }
}

